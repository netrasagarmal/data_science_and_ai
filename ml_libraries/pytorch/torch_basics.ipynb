{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa3797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68bd94ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3])\n",
    "\n",
    "tensor = torch.tensor(arr)\n",
    "\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b27832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],[4,5,6]])\n",
    "\n",
    "tensor = torch.tensor(arr)\n",
    "\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73051e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros((3,4))\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a2a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones((3,4))\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f985632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], dtype=torch.float16) torch.float16\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], dtype=torch.float16)\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]], dtype=torch.int32)\n",
      "tensor([[0.2108, 0.5876, 0.9157, 0.7846],\n",
      "        [0.8629, 0.8118, 0.6189, 0.0231],\n",
      "        [0.6050, 0.9401, 0.4860, 0.5094]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# default dtype is float32\n",
    "tensor = torch.zeros((3,4),dtype=torch.float16)\n",
    "print(tensor, tensor.dtype)\n",
    "\n",
    "# we can also manually set the dtype\n",
    "tensor = torch.zeros((3,4), dtype=torch.float16)\n",
    "print(tensor)\n",
    "tensor = torch.ones((3,4), dtype=torch.int32)\n",
    "print(tensor)\n",
    "tensor = torch.rand((3,4), dtype = torch.double)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb8dae",
   "metadata": {},
   "source": [
    "### Shape, Size, and Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00474e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "6\n",
      "tensor([[0.5881, 0.8420],\n",
      "        [0.8792, 0.6873],\n",
      "        [0.2656, 0.0879]])\n",
      "tensor([[0.5881, 0.8420],\n",
      "        [0.8792, 0.6873],\n",
      "        [0.2656, 0.0879]])\n",
      "torch.Size([1, 2, 3])\n",
      "torch.Size([2, 1, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "\n",
    "# Shape\n",
    "print(a.shape)          # torch.Size([2, 3])\n",
    "print(a.size())         # same\n",
    "\n",
    "# Number of elements\n",
    "print(a.numel())        # 6\n",
    "\n",
    "# Reshape\n",
    "print(a.view(3, 2))\n",
    "print(a.reshape(3, 2))  # safer than view\n",
    "\n",
    "# Add new dimension\n",
    "print(a.unsqueeze(0).shape)   # (1, 2, 3)\n",
    "print(a.unsqueeze(1).shape)   # (2, 1, 3)\n",
    "\n",
    "# Remove dimension\n",
    "b = a.unsqueeze(0)         # shape (1, 2, 3)\n",
    "print(b.squeeze().shape)   # (2, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c77af",
   "metadata": {},
   "source": [
    "### Basic Math Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34f90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.ones(2, 3)\n",
    "\n",
    "# Addition\n",
    "c = a + b\n",
    "c = torch.add(a, b)\n",
    "\n",
    "# Subtraction\n",
    "d = a - b\n",
    "d = torch.sub(a, b)\n",
    "\n",
    "# Multiplication (element wise)\n",
    "e = a * b\n",
    "e = torch.mul(a, b)\n",
    "\n",
    "# Division\n",
    "f = a / b\n",
    "f = torch.div(a, b)\n",
    "\n",
    "# Power\n",
    "p = a ** 2\n",
    "p = torch.pow(a, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186714b8",
   "metadata": {},
   "source": [
    "### In-place Operations (modifies tensor)\n",
    "\n",
    "All in-place functions end with `_`.\n",
    "\n",
    "‚ö† In-place ops modify data that may be required for autograd‚Äîuse carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52131b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9711, 0.9956, 0.9975],\n",
       "        [0.9967, 0.9917, 0.9936]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "\n",
    "a.add_(1)\n",
    "a.mul_(2)\n",
    "a.tanh_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b156e",
   "metadata": {},
   "source": [
    "### Reduction Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f98481e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4857)\n",
      "tensor([1.4753, 0.8457, 1.1647])\n",
      "tensor([2.3273, 1.1584])\n",
      "tensor(0.5809)\n",
      "tensor([0.7377, 0.4229, 0.5823])\n",
      "tensor([0.2078, 0.4821])\n",
      "tensor(0.0120) tensor(0.9484)\n",
      "torch.return_types.min(\n",
      "values=tensor([0.5451, 0.0120]),\n",
      "indices=tensor([0, 1]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "\n",
    "print(a.sum())                # all elements\n",
    "print(a.sum(dim=0))           # column-wise\n",
    "print(a.sum(dim=1))           # row-wise\n",
    "\n",
    "print(a.mean())               # global mean\n",
    "print(a.mean(dim=0))          # column mean\n",
    "\n",
    "print(a.std(dim=1))           # row std dev\n",
    "\n",
    "print(a.min(), a.max())       # global min, max\n",
    "print(a.min(dim=1))           # returns (values, indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a146b",
   "metadata": {},
   "source": [
    "### Matrix / Linear Algebra Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b26a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8458, 0.3698],\n",
      "        [0.9661, 0.7869],\n",
      "        [0.5213, 0.0477]])\n",
      "tensor(0.0048)\n",
      "tensor([[  3.0583,  -9.7016,  23.2013],\n",
      "        [  5.5415,  -7.3568,  11.0373],\n",
      "        [ -8.0961,  16.4368, -26.6580]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand(2, 3)\n",
    "B = torch.rand(3, 4)\n",
    "\n",
    "# Matrix multiplication\n",
    "C = torch.matmul(A, B)\n",
    "C = A @ B\n",
    "\n",
    "# Transpose\n",
    "print(A.t())         # swap last 2 dims\n",
    "\n",
    "# Matrix determinant (square matrices)\n",
    "M = torch.rand(3, 3)\n",
    "print(torch.det(M))\n",
    "\n",
    "# Matrix inverse\n",
    "print(torch.inverse(M))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfb360",
   "metadata": {},
   "source": [
    "### Concatenation & Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd014512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2986, 0.4258, 0.3560],\n",
      "        [0.3110, 0.1778, 0.4455],\n",
      "        [0.8671, 0.2082, 0.1517],\n",
      "        [0.3229, 0.7200, 0.1858]])\n",
      "tensor([[0.2986, 0.4258, 0.3560, 0.8671, 0.2082, 0.1517],\n",
      "        [0.3110, 0.1778, 0.4455, 0.3229, 0.7200, 0.1858]])\n",
      "tensor([[[0.2986, 0.4258, 0.3560],\n",
      "         [0.3110, 0.1778, 0.4455]],\n",
      "\n",
      "        [[0.8671, 0.2082, 0.1517],\n",
      "         [0.3229, 0.7200, 0.1858]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "y = torch.rand(2, 3)\n",
    "\n",
    "# Concatenate along rows (dim=0)\n",
    "print(torch.cat((x, y), dim=0))\n",
    "\n",
    "# Concatenate along columns (dim=1)\n",
    "print(torch.cat((x, y), dim=1))\n",
    "\n",
    "# Stack (creates new dimension)\n",
    "print(torch.stack((x, y), dim=0))  # shape (2, 2, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c5ae4",
   "metadata": {},
   "source": [
    "### Type Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc638ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1338, 0.8186, 0.8138],\n",
      "        [0.2892, 0.6650, 0.3367],\n",
      "        [0.6130, 0.3995, 0.5689]])\n",
      "tensor([[0.1338, 0.8186, 0.8138],\n",
      "        [0.2892, 0.6650, 0.3367],\n",
      "        [0.6130, 0.3995, 0.5689]], dtype=torch.float64)\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.int32)\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 3)\n",
    "\n",
    "print(a.float())\n",
    "print(a.double())\n",
    "print(a.int())\n",
    "print(a.bool())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d9882",
   "metadata": {},
   "source": [
    "### Device Movement (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d052507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 3)\n",
    "\n",
    "# Move to GPU\n",
    "# a_gpu = a.to(\"cuda\")\n",
    "\n",
    "# Move back\n",
    "# a_cpu = a_gpu.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4ea97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4986f0",
   "metadata": {},
   "source": [
    "### What is a *Variable* in PyTorch?\n",
    "\n",
    "Before PyTorch 0.4, you had to wrap tensors in `Variable` to compute gradients:\n",
    "\n",
    "```python\n",
    "from torch.autograd import Variable\n",
    "```\n",
    "\n",
    "But **now `Tensor` itself acts like a Variable**.\n",
    "\n",
    "üëâ So in modern PyTorch:\n",
    "\n",
    "**Every tensor with `requires_grad=True` is a Variable.**\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* PyTorch starts tracking all operations done on the tensor\n",
    "* It can compute gradients via backpropagation (`.backward()`)\n",
    "\n",
    "---\n",
    "\n",
    "### What is `requires_grad`?\n",
    "\n",
    "`requires_grad=True` tells PyTorch:\n",
    "\n",
    "> ‚ÄúTrack everything done on this tensor so I can compute gradients later.‚Äù\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ecedc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x * 3\n",
    "print(y)  # tensor(6., grad_fn=<MulBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e2b214",
   "metadata": {},
   "source": [
    "Notice the `grad_fn` ‚Äî it means the tensor is part of a computation graph.\n",
    "\n",
    "---\n",
    "\n",
    "### What is a Gradient?\n",
    "\n",
    "The **gradient** is the **derivative** of your output w.r.t your input.\n",
    "\n",
    "üëâ Used for training neural networks\n",
    "üëâ Used by optimizers like Adam, SGD\n",
    "üëâ Used for gradient descent to update weights\n",
    "\n",
    "If `y = x * 3` then:\n",
    "\n",
    "```\n",
    "dy/dx = 3\n",
    "```\n",
    "\n",
    "#### `backward()` Method\n",
    "The backward() method in Pytorch is used to calculate the gradient during the backward pass in the neural network. \n",
    "- If we <ins>do not call this backward()</ins> method then <ins>gradients are not calculated</ins> for the tensors.\n",
    "- The gradient of a tensor is calculated for the one having requires_grad is set to True. \n",
    "- We can access the gradients using .grad. \n",
    "- If we do not call the backward() method or even for the tensors whose requires_grad is set to False, the result is None.\n",
    "\n",
    "We can see this using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1eeec4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x * 3\n",
    "\n",
    "# Compute gradient\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)   # tensor(3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa2c0d",
   "metadata": {},
   "source": [
    "‚úî PyTorch automatically calculated dy/dx = 3\n",
    "\n",
    "---\n",
    "\n",
    "## Simple Example: y = x¬≤\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "```\n",
    "y = x^2\n",
    "dy/dx = 2x\n",
    "```\n",
    "\n",
    "Let‚Äôs check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da516cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)   # tensor(8.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428cf9ad",
   "metadata": {},
   "source": [
    "‚úî Correct ‚Üí 2 √ó 4 = 8\n",
    "\n",
    "---\n",
    "\n",
    "## Multiple Operations Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7471771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "y = x * 2          # 6\n",
    "z = y + 5          # 11\n",
    "out = z ** 2       # 121\n",
    "\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318a036",
   "metadata": {},
   "source": [
    "Math:\n",
    "\n",
    "```\n",
    "y = 2x\n",
    "z = y + 5 = 2x + 5\n",
    "out = (2x + 5)^2\n",
    "\n",
    "d(out)/dx = 2(2x+5)*2 = 4(2x+5)\n",
    "At x=3 ‚Üí 4(11) = 44\n",
    "```\n",
    "\n",
    "PyTorch prints:\n",
    "\n",
    "```\n",
    "tensor(44.)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Gradients only accumulate ‚Äî they don‚Äôt reset automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "833b75f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x * 3\n",
    "y.backward()\n",
    "print(x.grad)   # 3\n",
    "\n",
    "# Calling backward again without zeroing\n",
    "y = x * 3\n",
    "y.backward()\n",
    "print(x.grad)   # now 6 (accumulated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072dd0d",
   "metadata": {},
   "source": [
    "üî• Always clear gradients before next backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "babc5f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309c52f0",
   "metadata": {},
   "source": [
    "### `torch.no_grad()` ‚Äî turn off gradient tracking\n",
    "\n",
    "Used during:\n",
    "\n",
    "* inference\n",
    "* evaluation\n",
    "* updating tensors without tracking\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9917ec81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8., requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x += 3\n",
    "\n",
    "print(x)         # tensor(8.)\n",
    "print(x.requires_grad)    # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84c40e",
   "metadata": {},
   "source": [
    "Even though `x` has requires_grad=True, the operation inside `no_grad()` does not track history.\n",
    "\n",
    "---\n",
    "\n",
    "### Example with a small ‚Äúmodel weight‚Äù\n",
    "\n",
    "Imagine a single weight **w**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "907ee334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 25.0\n",
      "Gradient: tensor(-50.)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# simple prediction\n",
    "y_pred = w * 5    # y = 5w\n",
    "\n",
    "loss = (y_pred - 10)**2   # MSE loss\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Gradient:\", w.grad)  # tells us how to update w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e41a1a",
   "metadata": {},
   "source": [
    "This is exactly what happens inside every neural network layer.\n",
    "\n",
    "---\n",
    "\n",
    "### Manual gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc39dd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.0\n",
      "1 -14.0\n",
      "2 66.0\n",
      "3 -254.0\n",
      "4 1026.0\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "for i in range(5):\n",
    "    y_pred = w * 5\n",
    "    loss = (y_pred - 10)**2\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= 0.1 * w.grad   # gradient descent update\n",
    "    \n",
    "    w.grad.zero_()\n",
    "    \n",
    "    print(i, w.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4b7eb",
   "metadata": {},
   "source": [
    "This is the basic idea behind every optimizer like `SGD` or `Adam`.\n",
    "\n",
    "---\n",
    "`torch.no_grad()` is a **context manager** that tells PyTorch:\n",
    "\n",
    "**‚ÄúDisable gradient tracking inside this block.‚Äù**\n",
    "\n",
    "---\n",
    "\n",
    "### Why use `torch.no_grad()`?\n",
    "\n",
    "Because PyTorch normally tracks **every operation** on tensors with `requires_grad=True` to build a computation graph for backpropagation.\n",
    "\n",
    "But during:\n",
    "\n",
    "* **inference / prediction**\n",
    "* **evaluation**\n",
    "* **updating values manually**\n",
    "* **copying tensors**\n",
    "* **freezing model parameters**\n",
    "\n",
    "we **don‚Äôt want gradients** and **don‚Äôt want a computation graph**.\n",
    "\n",
    "So we wrap code in:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    # code here will NOT track gradients\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Simple Example\n",
    "\n",
    "##### Without `no_grad()`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x * 3\n",
    "print(y.grad_fn)   # <MulBackward0>\n",
    "```\n",
    "\n",
    "üîπ PyTorch tracked the operation\n",
    "üîπ A computation graph was created\n",
    "üîπ Memory is used\n",
    "üîπ Gradients will be computed during backward\n",
    "\n",
    "---\n",
    "\n",
    "##### With `torch.no_grad()`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x * 3\n",
    "\n",
    "print(y.grad_fn)   # None\n",
    "```\n",
    "\n",
    "‚úî No computation graph\n",
    "‚úî No gradient tracking\n",
    "‚úî Much less memory used\n",
    "‚úî Faster\n",
    "\n",
    "---\n",
    "\n",
    "### Why is it SUPER important?\n",
    "\n",
    "##### 1. **Inference is faster**\n",
    "\n",
    "No gradients ‚Üí no graph building ‚Üí lower memory & faster computation.\n",
    "\n",
    "##### 2. **Prevents unwanted gradient updates**\n",
    "\n",
    "When you update weights manually:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    w -= lr * w.grad\n",
    "```\n",
    "\n",
    "This ensures PyTorch **does not track** this update as part of the graph.\n",
    "\n",
    "##### 3. **Safe evaluation of validation data**\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_val)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### One-line definition\n",
    "\n",
    "**torch.no_grad() temporarily turns off autograd so operations inside it do not create gradients or computation graphs.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Summary (Very Simple)\n",
    "\n",
    "| Concept                            | Meaning                                  |\n",
    "| ---------------------------------- | ---------------------------------------- |\n",
    "| **Tensor with requires_grad=True** | A variable whose gradient is tracked     |\n",
    "| **grad**                           | Stores ‚àÇoutput/‚àÇinput                    |\n",
    "| **backward()**                     | Performs backpropagation                 |\n",
    "| **grad_fn**                        | Shows which operation created the tensor |\n",
    "| **no_grad()**                      | Turn off gradient tracking (inference)   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db76b4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
