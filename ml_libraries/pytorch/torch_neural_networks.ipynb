{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcff0aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b76e009",
   "metadata": {},
   "source": [
    "### Simple Neural Network Training Step\n",
    "\n",
    "- Initialize weights\n",
    "- Compute forward pass\n",
    "- Compute MSE loss\n",
    "- Backpropagate using `.backward()`\n",
    "- Manually update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1365a0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 39.6845\n",
      "Epoch 1, Loss = 6.5552\n",
      "Epoch 2, Loss = 1.1174\n",
      "Epoch 3, Loss = 0.3105\n",
      "Epoch 4, Loss = 0.2368\n"
     ]
    }
   ],
   "source": [
    "# ----- 1. Create sample data -----\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])   # shape (4,1)\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])   # y = 2x\n",
    "\n",
    "# ----- 2. Initialize weights -----\n",
    "w1 = torch.randn(1, 4, requires_grad=True)   # first layer weights\n",
    "b1 = torch.randn(4, requires_grad=True)\n",
    "\n",
    "w2 = torch.randn(4, 1, requires_grad=True)   # second layer weights\n",
    "b2 = torch.randn(1, requires_grad=True)\n",
    "\n",
    "lr = 0.01     # learning rate\n",
    "num_epochs = 5\n",
    "\n",
    "# ----- 3. Training loop -----\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # ----- Forward Pass -----\n",
    "    h = X @ w1 + b1        # Linear layer 1\n",
    "    h_relu = torch.relu(h) # Activation\n",
    "    y_pred = h_relu @ w2 + b2  # Linear layer 2\n",
    "\n",
    "    # ----- Loss MSE -----\n",
    "    loss = torch.mean((y_pred - y)**2)\n",
    "\n",
    "    # ----- Backward -----\n",
    "    loss.backward()\n",
    "\n",
    "    # ----- Update weights manually -----\n",
    "    with torch.no_grad():\n",
    "        w1 -= lr * w1.grad\n",
    "        b1 -= lr * b1.grad\n",
    "        w2 -= lr * w2.grad\n",
    "        b2 -= lr * b2.grad\n",
    "\n",
    "    # ----- Clear gradients -----\n",
    "    w1.grad.zero_()\n",
    "    b1.grad.zero_()\n",
    "    w2.grad.zero_()\n",
    "    b2.grad.zero_()\n",
    "\n",
    "    # if epoch % 10 == 0:\n",
    "    print(f\"Epoch {epoch}, Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d836f0da",
   "metadata": {},
   "source": [
    "### Neural Network Using Optimizer (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c7da6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 31.0984\n",
      "Epoch 1, Loss = 0.3288\n",
      "Epoch 2, Loss = 0.1788\n",
      "Epoch 3, Loss = 0.1720\n",
      "Epoch 4, Loss = 0.1676\n"
     ]
    }
   ],
   "source": [
    "# ----- 1. Sample Data -----\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# ----- 2. Initialize weights -----\n",
    "w1 = torch.randn(1, 4, requires_grad=True)\n",
    "b1 = torch.randn(4, requires_grad=True)\n",
    "\n",
    "w2 = torch.randn(4, 1, requires_grad=True)\n",
    "b2 = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Collect parameters for optimizer\n",
    "params = [w1, b1, w2, b2]\n",
    "\n",
    "# ----- 3. Optimizer (SGD) -----\n",
    "optimizer = torch.optim.SGD(params, lr=0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "# ----- 4. Training Loop -----\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Forward\n",
    "    h = X @ w1 + b1\n",
    "    h_relu = torch.relu(h)\n",
    "    y_pred = h_relu @ w2 + b2\n",
    "\n",
    "    # Loss\n",
    "    loss = torch.mean((y_pred - y)**2)\n",
    "\n",
    "    # Backprop\n",
    "    optimizer.zero_grad()   # clear old gradients\n",
    "    loss.backward()         # compute new gradients\n",
    "    optimizer.step()        # update weights\n",
    "\n",
    "    # if epoch % 10 == 0:\n",
    "    print(f\"Epoch {epoch}, Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d073a7",
   "metadata": {},
   "source": [
    "### What is `nn.Linear`?\n",
    "\n",
    "`nn.Linear(in_features, out_features)` performs a **linear transformation**:\n",
    "\n",
    "[\n",
    "y = xW^{T} + b\n",
    "]\n",
    "\n",
    "* **W** â†’ weight matrix of size `(out_features Ã— in_features)`\n",
    "* **b** â†’ bias vector of size `(out_features)`\n",
    "\n",
    "So if you call:\n",
    "\n",
    "```python\n",
    "layer = nn.Linear(3, 2)\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "* Input shape must be: `[batch_size, 3]`\n",
    "* Output shape will be: `[batch_size, 2]`\n",
    "\n",
    "PyTorch handles:\n",
    "âœ” Creating weights\n",
    "âœ” Creating bias\n",
    "âœ” Tracking gradients\n",
    "âœ” Updating parameters during training\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Using `nn.Linear` in Functional Style\n",
    "\n",
    "**No classes, everything step-by-step.**\n",
    "\n",
    "We build a tiny model:\n",
    "\n",
    "```\n",
    "X â†’ Linear(1â†’4) â†’ ReLU â†’ Linear(4â†’1) â†’ Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###**Code: Functional neural network using nn.Linear**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5fc79ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 39.7125\n",
      "Epoch 1, Loss = 26.2883\n",
      "Epoch 2, Loss = 17.5546\n",
      "Epoch 3, Loss = 10.1455\n",
      "Epoch 4, Loss = 4.5563\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1. Data\n",
    "# ---------------------------\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])   # Inputs\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])   # Targets (y = 2x)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define layers (Functional Style)\n",
    "# ---------------------------\n",
    "layer1 = nn.Linear(1, 4)   # input 1 -> hidden 4\n",
    "layer2 = nn.Linear(4, 1)   # hidden 4 -> output 1\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(list(layer1.parameters()) + \n",
    "                            list(layer2.parameters()), lr=0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "# ---------------------------\n",
    "# 3. Training Loop\n",
    "# ---------------------------\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # ----- Forward Pass -----\n",
    "    h = layer1(X)          # linear layer\n",
    "    h = F.relu(h)          # activation\n",
    "    y_pred = layer2(h)     # output layer\n",
    "\n",
    "    # ----- Loss -----\n",
    "    loss = F.mse_loss(y_pred, y)\n",
    "\n",
    "    # ----- Backward Pass -----\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if epoch % 10 == 0:\n",
    "    print(f\"Epoch {epoch}, Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e9c0e",
   "metadata": {},
   "source": [
    "### What this example shows\n",
    "\n",
    "##### âœ” Functional-style usage (no class)\n",
    "\n",
    "You manually create each layer:\n",
    "\n",
    "```python\n",
    "layer1 = nn.Linear(1, 4)\n",
    "layer2 = nn.Linear(4, 1)\n",
    "```\n",
    "\n",
    "##### âœ” Forward pass is explicit\n",
    "\n",
    "```python\n",
    "h = layer1(X)\n",
    "h = F.relu(h)\n",
    "y_pred = layer2(h)\n",
    "```\n",
    "\n",
    "##### âœ” Loss calculation\n",
    "\n",
    "```python\n",
    "loss = F.mse_loss(y_pred, y)\n",
    "```\n",
    "\n",
    "##### âœ” Backprop + update\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "Everything happens step-by-step so you can clearly see how the network works.\n",
    "\n",
    "---\n",
    "\n",
    "### Optional: Print weights and bias\n",
    "\n",
    "```python\n",
    "print(layer1.weight)\n",
    "print(layer1.bias)\n",
    "```\n",
    "\n",
    "You'll see they update each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a29537",
   "metadata": {},
   "source": [
    "### What is `nn.Sequential`?\n",
    "\n",
    "`nn.Sequential` lets you **stack layers in order** like this:\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "```\n",
    "\n",
    "It automatically connects output â†’ next input â†’ next output.\n",
    "\n",
    "You only define the architecture once, and call:\n",
    "\n",
    "```python\n",
    "y_pred = model(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Neural Network with `nn.Sequential` (Functional Style)\n",
    "\n",
    "We build:\n",
    "\n",
    "```\n",
    "X â†’ Linear(1â†’4) â†’ ReLU â†’ Linear(4â†’1) â†’ Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5395cbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 30.1275\n",
      "Epoch 1, Loss = 23.3221\n",
      "Epoch 2, Loss = 17.4129\n",
      "Epoch 3, Loss = 11.5881\n",
      "Epoch 4, Loss = 6.3862\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1. Data\n",
    "# ---------------------------\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])   # y = 2x\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define model using nn.Sequential\n",
    "# ---------------------------\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 4),   # layer 1\n",
    "    nn.ReLU(),         # activation\n",
    "    nn.Linear(4, 1)    # layer 2\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "# ---------------------------\n",
    "# 3. Training Loop\n",
    "# ---------------------------\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Forward Pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # Loss\n",
    "    loss = F.mse_loss(y_pred, y)\n",
    "\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if epoch % 10 == 0:\n",
    "    print(f\"Epoch {epoch}, Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b70ec4",
   "metadata": {},
   "source": [
    "\n",
    "### Why `nn.Sequential` is helpful?\n",
    "\n",
    "##### âœ” Cleaner model definition\n",
    "\n",
    "No need to manually call each layer.\n",
    "\n",
    "##### âœ” Easy to add/remove layers\n",
    "\n",
    "Just modify the list.\n",
    "\n",
    "##### âœ” Perfect for simple feedforward networks\n",
    "\n",
    "Especially MLPs, CNN blocks, etc.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 1)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086a6940",
   "metadata": {},
   "source": [
    "Below are **three clean, minimal, class-based PyTorch model examples**, one for each style:\n",
    "\n",
    "1. **Very simple NN (fully manual forward calculations)**\n",
    "2. **Class-based model using `nn.Linear` layers**\n",
    "3. **Class-based model using `nn.Sequential`**\n",
    "\n",
    "All examples include:\n",
    "\n",
    "* A class defining the model\n",
    "* A forward function\n",
    "* Simple training loop template\n",
    "\n",
    "This will give you a **clear understanding of how PyTorch models are structured under the hood**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Class-Based Model: Very First Simple NN (Manual Weights)**\n",
    "\n",
    "Here we **manually create weights and do matrix multiplications ourselves**.\n",
    "\n",
    "##### ðŸ‘‰ Best for: understanding how PyTorch really works under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "659b3b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0261],\n",
      "        [-2.0660],\n",
      "        [-2.1126],\n",
      "        [-2.3648]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Simple model with manually created parameters\n",
    "# ---------------------------------------------------\n",
    "class SimpleManualNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Manually define weights and biases\n",
    "        self.w1 = nn.Parameter(torch.randn(1, 4))\n",
    "        self.b1 = nn.Parameter(torch.randn(4))\n",
    "\n",
    "        self.w2 = nn.Parameter(torch.randn(4, 1))\n",
    "        self.b2 = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x @ self.w1 + self.b1    # Linear layer 1\n",
    "        h = F.relu(h)                # Activation\n",
    "        out = h @ self.w2 + self.b2  # Linear layer 2\n",
    "        return out\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = SimpleManualNN()\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_pred = model(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c89fc62",
   "metadata": {},
   "source": [
    "âœ” Manually implemented linear layers\n",
    "âœ” Useful for learning core mechanics\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Class-Based Model Using `nn.Linear` (Proper PyTorch Way)**\n",
    "\n",
    "The cleanest and most common architecture structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516d78dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5326],\n",
      "        [-0.6009],\n",
      "        [-0.6692],\n",
      "        [-0.7375]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Class model using nn.Linear layers\n",
    "# ---------------------------------------------------\n",
    "class LinearNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, 4)   # input â†’ hidden\n",
    "        self.fc2 = nn.Linear(4, 1)   # hidden â†’ output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = LinearNN()\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_pred = model(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f24447",
   "metadata": {},
   "source": [
    "âœ” Clean and flexible\n",
    "âœ” Easy to expand with more layers\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Class-Based Model Using `nn.Sequential`**\n",
    "\n",
    "Here, the entire network is wrapped inside one Sequential block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "663e973b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5038],\n",
      "        [0.6392],\n",
      "        [0.7709],\n",
      "        [0.8968]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Class model using nn.Sequential\n",
    "# ---------------------------------------------------\n",
    "class SequentialNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = SequentialNN()\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_pred = model(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb5827",
   "metadata": {},
   "source": [
    "âœ” Very compact\n",
    "âœ” Perfect for simple feedforward architectures\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Loop Template (Works for All 3 Models)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d90268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 43.9718\n",
      "Epoch 1 Loss: 27.0567\n",
      "Epoch 2 Loss: 16.9035\n",
      "Epoch 3 Loss: 8.8359\n",
      "Epoch 4 Loss: 3.4723\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "model = LinearNN()        # or SimpleManualNN(), SequentialNN()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 5\n",
    "# ---------------------------\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # if epoch % 10 == 0:\n",
    "    print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58786b62",
   "metadata": {},
   "source": [
    "### Summary Table (Easy Understanding)\n",
    "\n",
    "| Model Type         | How It's Built                                  | Best For                          |\n",
    "| ------------------ | ----------------------------------------------- | --------------------------------- |\n",
    "| **SimpleManualNN** | Manual weights (`nn.Parameter`) + manual matmul | Understanding internals           |\n",
    "| **LinearNN**       | Uses `nn.Linear` layers                         | Standard choice for most networks |\n",
    "| **SequentialNN**   | Uses `nn.Sequential` container                  | Very compact CNN/MLP blocks       |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
