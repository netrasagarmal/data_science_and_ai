{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa3797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68bd94ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([1, 2, 3])\n",
    "\n",
    "tensor = torch.tensor(arr)\n",
    "\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b27832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([[1, 2, 3],[4,5,6]])\n",
    "\n",
    "tensor = torch.tensor(arr)\n",
    "\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73051e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros((3,4))\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a2a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones((3,4))\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f985632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], dtype=torch.float16) torch.float16\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]], dtype=torch.float16)\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]], dtype=torch.int32)\n",
      "tensor([[0.2108, 0.5876, 0.9157, 0.7846],\n",
      "        [0.8629, 0.8118, 0.6189, 0.0231],\n",
      "        [0.6050, 0.9401, 0.4860, 0.5094]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# default dtype is float32\n",
    "tensor = torch.zeros((3,4),dtype=torch.float16)\n",
    "print(tensor, tensor.dtype)\n",
    "\n",
    "# we can also manually set the dtype\n",
    "tensor = torch.zeros((3,4), dtype=torch.float16)\n",
    "print(tensor)\n",
    "tensor = torch.ones((3,4), dtype=torch.int32)\n",
    "print(tensor)\n",
    "tensor = torch.rand((3,4), dtype = torch.double)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb8dae",
   "metadata": {},
   "source": [
    "### Shape, Size, and Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00474e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "6\n",
      "tensor([[0.5881, 0.8420],\n",
      "        [0.8792, 0.6873],\n",
      "        [0.2656, 0.0879]])\n",
      "tensor([[0.5881, 0.8420],\n",
      "        [0.8792, 0.6873],\n",
      "        [0.2656, 0.0879]])\n",
      "torch.Size([1, 2, 3])\n",
      "torch.Size([2, 1, 3])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "\n",
    "# Shape\n",
    "print(a.shape)          # torch.Size([2, 3])\n",
    "print(a.size())         # same\n",
    "\n",
    "# Number of elements\n",
    "print(a.numel())        # 6\n",
    "\n",
    "# Reshape\n",
    "print(a.view(3, 2))\n",
    "print(a.reshape(3, 2))  # safer than view\n",
    "\n",
    "# Add new dimension\n",
    "print(a.unsqueeze(0).shape)   # (1, 2, 3)\n",
    "print(a.unsqueeze(1).shape)   # (2, 1, 3)\n",
    "\n",
    "# Remove dimension\n",
    "b = a.unsqueeze(0)         # shape (1, 2, 3)\n",
    "print(b.squeeze().shape)   # (2, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5c77af",
   "metadata": {},
   "source": [
    "### Basic Math Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34f90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.ones(2, 3)\n",
    "\n",
    "# Addition\n",
    "c = a + b\n",
    "c = torch.add(a, b)\n",
    "\n",
    "# Subtraction\n",
    "d = a - b\n",
    "d = torch.sub(a, b)\n",
    "\n",
    "# Multiplication (element wise)\n",
    "e = a * b\n",
    "e = torch.mul(a, b)\n",
    "\n",
    "# Division\n",
    "f = a / b\n",
    "f = torch.div(a, b)\n",
    "\n",
    "# Power\n",
    "p = a ** 2\n",
    "p = torch.pow(a, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186714b8",
   "metadata": {},
   "source": [
    "### In-place Operations (modifies tensor)\n",
    "\n",
    "All in-place functions end with `_`.\n",
    "\n",
    "‚ö† In-place ops modify data that may be required for autograd‚Äîuse carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52131b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9711, 0.9956, 0.9975],\n",
       "        [0.9967, 0.9917, 0.9936]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "\n",
    "a.add_(1)\n",
    "a.mul_(2)\n",
    "a.tanh_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79b156e",
   "metadata": {},
   "source": [
    "### Reduction Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f98481e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4857)\n",
      "tensor([1.4753, 0.8457, 1.1647])\n",
      "tensor([2.3273, 1.1584])\n",
      "tensor(0.5809)\n",
      "tensor([0.7377, 0.4229, 0.5823])\n",
      "tensor([0.2078, 0.4821])\n",
      "tensor(0.0120) tensor(0.9484)\n",
      "torch.return_types.min(\n",
      "values=tensor([0.5451, 0.0120]),\n",
      "indices=tensor([0, 1]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "\n",
    "print(a.sum())                # all elements\n",
    "print(a.sum(dim=0))           # column-wise\n",
    "print(a.sum(dim=1))           # row-wise\n",
    "\n",
    "print(a.mean())               # global mean\n",
    "print(a.mean(dim=0))          # column mean\n",
    "\n",
    "print(a.std(dim=1))           # row std dev\n",
    "\n",
    "print(a.min(), a.max())       # global min, max\n",
    "print(a.min(dim=1))           # returns (values, indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54a146b",
   "metadata": {},
   "source": [
    "### Matrix / Linear Algebra Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b26a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8458, 0.3698],\n",
      "        [0.9661, 0.7869],\n",
      "        [0.5213, 0.0477]])\n",
      "tensor(0.0048)\n",
      "tensor([[  3.0583,  -9.7016,  23.2013],\n",
      "        [  5.5415,  -7.3568,  11.0373],\n",
      "        [ -8.0961,  16.4368, -26.6580]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand(2, 3)\n",
    "B = torch.rand(3, 4)\n",
    "\n",
    "# Matrix multiplication\n",
    "C = torch.matmul(A, B)\n",
    "C = A @ B\n",
    "\n",
    "# Transpose\n",
    "print(A.t())         # swap last 2 dims\n",
    "\n",
    "# Matrix determinant (square matrices)\n",
    "M = torch.rand(3, 3)\n",
    "print(torch.det(M))\n",
    "\n",
    "# Matrix inverse\n",
    "print(torch.inverse(M))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfb360",
   "metadata": {},
   "source": [
    "### Concatenation & Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd014512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2986, 0.4258, 0.3560],\n",
      "        [0.3110, 0.1778, 0.4455],\n",
      "        [0.8671, 0.2082, 0.1517],\n",
      "        [0.3229, 0.7200, 0.1858]])\n",
      "tensor([[0.2986, 0.4258, 0.3560, 0.8671, 0.2082, 0.1517],\n",
      "        [0.3110, 0.1778, 0.4455, 0.3229, 0.7200, 0.1858]])\n",
      "tensor([[[0.2986, 0.4258, 0.3560],\n",
      "         [0.3110, 0.1778, 0.4455]],\n",
      "\n",
      "        [[0.8671, 0.2082, 0.1517],\n",
      "         [0.3229, 0.7200, 0.1858]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3)\n",
    "y = torch.rand(2, 3)\n",
    "\n",
    "# Concatenate along rows (dim=0)\n",
    "print(torch.cat((x, y), dim=0))\n",
    "\n",
    "# Concatenate along columns (dim=1)\n",
    "print(torch.cat((x, y), dim=1))\n",
    "\n",
    "# Stack (creates new dimension)\n",
    "print(torch.stack((x, y), dim=0))  # shape (2, 2, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c5ae4",
   "metadata": {},
   "source": [
    "### Type Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc638ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1338, 0.8186, 0.8138],\n",
      "        [0.2892, 0.6650, 0.3367],\n",
      "        [0.6130, 0.3995, 0.5689]])\n",
      "tensor([[0.1338, 0.8186, 0.8138],\n",
      "        [0.2892, 0.6650, 0.3367],\n",
      "        [0.6130, 0.3995, 0.5689]], dtype=torch.float64)\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]], dtype=torch.int32)\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 3)\n",
    "\n",
    "print(a.float())\n",
    "print(a.double())\n",
    "print(a.int())\n",
    "print(a.bool())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8d9882",
   "metadata": {},
   "source": [
    "### Device Movement (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d052507c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 3)\n",
    "\n",
    "# Move to GPU\n",
    "# a_gpu = a.to(\"cuda\")\n",
    "\n",
    "# Move back\n",
    "# a_cpu = a_gpu.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4ea97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4986f0",
   "metadata": {},
   "source": [
    "## What is a *Variable* in PyTorch?\n",
    "\n",
    "Before PyTorch 0.4, you had to wrap tensors in `Variable` to compute gradients:\n",
    "\n",
    "```python\n",
    "from torch.autograd import Variable\n",
    "```\n",
    "\n",
    "But **now `Tensor` itself acts like a Variable**.\n",
    "\n",
    "üëâ So in modern PyTorch:\n",
    "\n",
    "**Every tensor with `requires_grad=True` is a Variable.**\n",
    "\n",
    "Meaning:\n",
    "\n",
    "* PyTorch starts tracking all operations done on the tensor\n",
    "* It can compute gradients via backpropagation (`.backward()`)\n",
    "\n",
    "---\n",
    "\n",
    "## What is `requires_grad`?\n",
    "\n",
    "`requires_grad=True` tells PyTorch:\n",
    "\n",
    "> ‚ÄúTrack everything done on this tensor so I can compute gradients later.‚Äù\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ecedc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x * 3\n",
    "print(y)  # tensor(6., grad_fn=<MulBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e2b214",
   "metadata": {},
   "source": [
    "Notice the `grad_fn` ‚Äî it means the tensor is part of a computation graph.\n",
    "\n",
    "---\n",
    "\n",
    "## What is a Gradient?\n",
    "\n",
    "The **gradient** is the **derivative** of your output w.r.t your input.\n",
    "\n",
    "üëâ Used for training neural networks\n",
    "üëâ Used by optimizers like Adam, SGD\n",
    "üëâ Used for gradient descent to update weights\n",
    "\n",
    "If `y = x * 3` then:\n",
    "\n",
    "```\n",
    "dy/dx = 3\n",
    "```\n",
    "\n",
    "We can see this using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1eeec4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x * 3\n",
    "\n",
    "# Compute gradient\n",
    "y.backward()\n",
    "\n",
    "print(x.grad)   # tensor(3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa2c0d",
   "metadata": {},
   "source": [
    "‚úî PyTorch automatically calculated dy/dx = 3\n",
    "\n",
    "---\n",
    "\n",
    "## Simple Example: y = x¬≤\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "```\n",
    "y = x^2\n",
    "dy/dx = 2x\n",
    "```\n",
    "\n",
    "Let‚Äôs check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da516cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "y = x ** 2\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)   # tensor(8.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428cf9ad",
   "metadata": {},
   "source": [
    "‚úî Correct ‚Üí 2 √ó 4 = 8\n",
    "\n",
    "---\n",
    "\n",
    "## Multiple Operations Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7471771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "y = x * 2          # 6\n",
    "z = y + 5          # 11\n",
    "out = z ** 2       # 121\n",
    "\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f318a036",
   "metadata": {},
   "source": [
    "Math:\n",
    "\n",
    "```\n",
    "y = 2x\n",
    "z = y + 5 = 2x + 5\n",
    "out = (2x + 5)^2\n",
    "\n",
    "d(out)/dx = 2(2x+5)*2 = 4(2x+5)\n",
    "At x=3 ‚Üí 4(11) = 44\n",
    "```\n",
    "\n",
    "PyTorch prints:\n",
    "\n",
    "```\n",
    "tensor(44.)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Gradients only accumulate ‚Äî they don‚Äôt reset automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "833b75f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x * 3\n",
    "y.backward()\n",
    "print(x.grad)   # 3\n",
    "\n",
    "# Calling backward again without zeroing\n",
    "y = x * 3\n",
    "y.backward()\n",
    "print(x.grad)   # now 6 (accumulated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072dd0d",
   "metadata": {},
   "source": [
    "üî• Always clear gradients before next backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "babc5f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309c52f0",
   "metadata": {},
   "source": [
    "## `torch.no_grad()` ‚Äî turn off gradient tracking\n",
    "\n",
    "Used during:\n",
    "\n",
    "* inference\n",
    "* evaluation\n",
    "* updating tensors without tracking\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9917ec81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8., requires_grad=True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x += 3\n",
    "\n",
    "print(x)         # tensor(8.)\n",
    "print(x.requires_grad)    # True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd84c40e",
   "metadata": {},
   "source": [
    "Even though `x` has requires_grad=True, the operation inside `no_grad()` does not track history.\n",
    "\n",
    "---\n",
    "\n",
    "## Example with a small ‚Äúmodel weight‚Äù\n",
    "\n",
    "Imagine a single weight **w**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "907ee334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 25.0\n",
      "Gradient: tensor(-50.)\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# simple prediction\n",
    "y_pred = w * 5    # y = 5w\n",
    "\n",
    "loss = (y_pred - 10)**2   # MSE loss\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Gradient:\", w.grad)  # tells us how to update w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e41a1a",
   "metadata": {},
   "source": [
    "This is exactly what happens inside every neural network layer.\n",
    "\n",
    "---\n",
    "\n",
    "## Manual gradient descent (for beginners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bc39dd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.0\n",
      "1 -14.0\n",
      "2 66.0\n",
      "3 -254.0\n",
      "4 1026.0\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "for i in range(5):\n",
    "    y_pred = w * 5\n",
    "    loss = (y_pred - 10)**2\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= 0.1 * w.grad   # gradient descent update\n",
    "    \n",
    "    w.grad.zero_()\n",
    "    \n",
    "    print(i, w.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4b7eb",
   "metadata": {},
   "source": [
    "This is the basic idea behind every optimizer like `SGD` or `Adam`.\n",
    "\n",
    "---\n",
    "`torch.no_grad()` is a **context manager** that tells PyTorch:\n",
    "\n",
    "**‚ÄúDisable gradient tracking inside this block.‚Äù**\n",
    "\n",
    "---\n",
    "\n",
    "### Why use `torch.no_grad()`?\n",
    "\n",
    "Because PyTorch normally tracks **every operation** on tensors with `requires_grad=True` to build a computation graph for backpropagation.\n",
    "\n",
    "But during:\n",
    "\n",
    "* **inference / prediction**\n",
    "* **evaluation**\n",
    "* **updating values manually**\n",
    "* **copying tensors**\n",
    "* **freezing model parameters**\n",
    "\n",
    "we **don‚Äôt want gradients** and **don‚Äôt want a computation graph**.\n",
    "\n",
    "So we wrap code in:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    # code here will NOT track gradients\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Simple Example\n",
    "\n",
    "##### Without `no_grad()`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x * 3\n",
    "print(y.grad_fn)   # <MulBackward0>\n",
    "```\n",
    "\n",
    "üîπ PyTorch tracked the operation\n",
    "üîπ A computation graph was created\n",
    "üîπ Memory is used\n",
    "üîπ Gradients will be computed during backward\n",
    "\n",
    "---\n",
    "\n",
    "##### With `torch.no_grad()`\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x * 3\n",
    "\n",
    "print(y.grad_fn)   # None\n",
    "```\n",
    "\n",
    "‚úî No computation graph\n",
    "‚úî No gradient tracking\n",
    "‚úî Much less memory used\n",
    "‚úî Faster\n",
    "\n",
    "---\n",
    "\n",
    "### Why is it SUPER important?\n",
    "\n",
    "##### 1. **Inference is faster**\n",
    "\n",
    "No gradients ‚Üí no graph building ‚Üí lower memory & faster computation.\n",
    "\n",
    "##### 2. **Prevents unwanted gradient updates**\n",
    "\n",
    "When you update weights manually:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    w -= lr * w.grad\n",
    "```\n",
    "\n",
    "This ensures PyTorch **does not track** this update as part of the graph.\n",
    "\n",
    "##### 3. **Safe evaluation of validation data**\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_val)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### One-line definition\n",
    "\n",
    "**torch.no_grad() temporarily turns off autograd so operations inside it do not create gradients or computation graphs.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Summary (Very Simple)\n",
    "\n",
    "| Concept                            | Meaning                                  |\n",
    "| ---------------------------------- | ---------------------------------------- |\n",
    "| **Tensor with requires_grad=True** | A variable whose gradient is tracked     |\n",
    "| **grad**                           | Stores ‚àÇoutput/‚àÇinput                    |\n",
    "| **backward()**                     | Performs backpropagation                 |\n",
    "| **grad_fn**                        | Shows which operation created the tensor |\n",
    "| **no_grad()**                      | Turn off gradient tracking (inference)   |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a80ff2",
   "metadata": {},
   "source": [
    "### Simple Neural Network Training Step\n",
    "\n",
    "- Initialize weights\n",
    "- Compute forward pass\n",
    "- Compute MSE loss\n",
    "- Backpropagate using `.backward()`\n",
    "- Manually update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7a75f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 20.6364\n"
     ]
    }
   ],
   "source": [
    "# ----- 1. Create sample data -----\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])   # shape (4,1)\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])   # y = 2x\n",
    "\n",
    "# ----- 2. Initialize weights -----\n",
    "w1 = torch.randn(1, 4, requires_grad=True)   # first layer weights\n",
    "b1 = torch.randn(4, requires_grad=True)\n",
    "\n",
    "w2 = torch.randn(4, 1, requires_grad=True)   # second layer weights\n",
    "b2 = torch.randn(1, requires_grad=True)\n",
    "\n",
    "lr = 0.01     # learning rate\n",
    "num_epochs = 5\n",
    "\n",
    "# ----- 3. Training loop -----\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # ----- Forward Pass -----\n",
    "    h = X @ w1 + b1        # Linear layer 1\n",
    "    h_relu = torch.relu(h) # Activation\n",
    "    y_pred = h_relu @ w2 + b2  # Linear layer 2\n",
    "\n",
    "    # ----- Loss -----\n",
    "    loss = torch.mean((y_pred - y)**2)\n",
    "\n",
    "    # ----- Backward -----\n",
    "    loss.backward()\n",
    "\n",
    "    # ----- Update weights manually -----\n",
    "    with torch.no_grad():\n",
    "        w1 -= lr * w1.grad\n",
    "        b1 -= lr * b1.grad\n",
    "        w2 -= lr * w2.grad\n",
    "        b2 -= lr * b2.grad\n",
    "\n",
    "    # ----- Clear gradients -----\n",
    "    w1.grad.zero_()\n",
    "    b1.grad.zero_()\n",
    "    w2.grad.zero_()\n",
    "    b2.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942e6cd",
   "metadata": {},
   "source": [
    "### Neural Network Using Optimizer (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d2e0f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 0.4339\n"
     ]
    }
   ],
   "source": [
    "# ----- 1. Sample Data -----\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "# ----- 2. Initialize weights -----\n",
    "w1 = torch.randn(1, 4, requires_grad=True)\n",
    "b1 = torch.randn(4, requires_grad=True)\n",
    "\n",
    "w2 = torch.randn(4, 1, requires_grad=True)\n",
    "b2 = torch.randn(1, requires_grad=True)\n",
    "\n",
    "# Collect parameters for optimizer\n",
    "params = [w1, b1, w2, b2]\n",
    "\n",
    "# ----- 3. Optimizer (SGD) -----\n",
    "optimizer = torch.optim.SGD(params, lr=0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "# ----- 4. Training Loop -----\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Forward\n",
    "    h = X @ w1 + b1\n",
    "    h_relu = torch.relu(h)\n",
    "    y_pred = h_relu @ w2 + b2\n",
    "\n",
    "    # Loss\n",
    "    loss = torch.mean((y_pred - y)**2)\n",
    "\n",
    "    # Backprop\n",
    "    optimizer.zero_grad()   # clear old gradients\n",
    "    loss.backward()         # compute new gradients\n",
    "    optimizer.step()        # update weights\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d353568",
   "metadata": {},
   "source": [
    "### What is `nn.Linear`?\n",
    "\n",
    "`nn.Linear(in_features, out_features)` performs a **linear transformation**:\n",
    "\n",
    "[\n",
    "y = xW^{T} + b\n",
    "]\n",
    "\n",
    "* **W** ‚Üí weight matrix of size `(out_features √ó in_features)`\n",
    "* **b** ‚Üí bias vector of size `(out_features)`\n",
    "\n",
    "So if you call:\n",
    "\n",
    "```python\n",
    "layer = nn.Linear(3, 2)\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "* Input shape must be: `[batch_size, 3]`\n",
    "* Output shape will be: `[batch_size, 2]`\n",
    "\n",
    "PyTorch handles:\n",
    "‚úî Creating weights\n",
    "‚úî Creating bias\n",
    "‚úî Tracking gradients\n",
    "‚úî Updating parameters during training\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Using `nn.Linear` in Functional Style\n",
    "\n",
    "**No classes, everything step-by-step.**\n",
    "\n",
    "We build a tiny model:\n",
    "\n",
    "```\n",
    "X ‚Üí Linear(1‚Üí4) ‚Üí ReLU ‚Üí Linear(4‚Üí1) ‚Üí Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "###**Code: Functional neural network using nn.Linear**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "404fd025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 40.9530\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1. Data\n",
    "# ---------------------------\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])   # Inputs\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])   # Targets (y = 2x)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define layers (Functional Style)\n",
    "# ---------------------------\n",
    "layer1 = nn.Linear(1, 4)   # input 1 -> hidden 4\n",
    "layer2 = nn.Linear(4, 1)   # hidden 4 -> output 1\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(list(layer1.parameters()) + \n",
    "                            list(layer2.parameters()), lr=0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "# ---------------------------\n",
    "# 3. Training Loop\n",
    "# ---------------------------\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # ----- Forward Pass -----\n",
    "    h = layer1(X)          # linear layer\n",
    "    h = F.relu(h)          # activation\n",
    "    y_pred = layer2(h)     # output layer\n",
    "\n",
    "    # ----- Loss -----\n",
    "    loss = F.mse_loss(y_pred, y)\n",
    "\n",
    "    # ----- Backward Pass -----\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb270a",
   "metadata": {},
   "source": [
    "### What this example shows\n",
    "\n",
    "##### ‚úî Functional-style usage (no class)\n",
    "\n",
    "You manually create each layer:\n",
    "\n",
    "```python\n",
    "layer1 = nn.Linear(1, 4)\n",
    "layer2 = nn.Linear(4, 1)\n",
    "```\n",
    "\n",
    "##### ‚úî Forward pass is explicit\n",
    "\n",
    "```python\n",
    "h = layer1(X)\n",
    "h = F.relu(h)\n",
    "y_pred = layer2(h)\n",
    "```\n",
    "\n",
    "##### ‚úî Loss calculation\n",
    "\n",
    "```python\n",
    "loss = F.mse_loss(y_pred, y)\n",
    "```\n",
    "\n",
    "##### ‚úî Backprop + update\n",
    "\n",
    "```python\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "Everything happens step-by-step so you can clearly see how the network works.\n",
    "\n",
    "---\n",
    "\n",
    "### Optional: Print weights and bias\n",
    "\n",
    "```python\n",
    "print(layer1.weight)\n",
    "print(layer1.bias)\n",
    "```\n",
    "\n",
    "You'll see they update each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb7fdc3",
   "metadata": {},
   "source": [
    "### What is `nn.Sequential`?\n",
    "\n",
    "`nn.Sequential` lets you **stack layers in order** like this:\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 1)\n",
    ")\n",
    "```\n",
    "\n",
    "It automatically connects output ‚Üí next input ‚Üí next output.\n",
    "\n",
    "You only define the architecture once, and call:\n",
    "\n",
    "```python\n",
    "y_pred = model(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Neural Network with `nn.Sequential` (Functional Style)\n",
    "\n",
    "We build:\n",
    "\n",
    "```\n",
    "X ‚Üí Linear(1‚Üí4) ‚Üí ReLU ‚Üí Linear(4‚Üí1) ‚Üí Output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a38d2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss = 34.9010\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1. Data\n",
    "# ---------------------------\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])   # y = 2x\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Define model using nn.Sequential\n",
    "# ---------------------------\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 4),   # layer 1\n",
    "    nn.ReLU(),         # activation\n",
    "    nn.Linear(4, 1)    # layer 2\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 5\n",
    "# ---------------------------\n",
    "# 3. Training Loop\n",
    "# ---------------------------\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Forward Pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    # Loss\n",
    "    loss = F.mse_loss(y_pred, y)\n",
    "\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f164789",
   "metadata": {},
   "source": [
    "\n",
    "### Why `nn.Sequential` is helpful?\n",
    "\n",
    "##### ‚úî Cleaner model definition\n",
    "\n",
    "No need to manually call each layer.\n",
    "\n",
    "##### ‚úî Easy to add/remove layers\n",
    "\n",
    "Just modify the list.\n",
    "\n",
    "##### ‚úî Perfect for simple feedforward networks\n",
    "\n",
    "Especially MLPs, CNN blocks, etc.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(10, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 1)\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10717569",
   "metadata": {},
   "source": [
    "Below are **three clean, minimal, class-based PyTorch model examples**, one for each style:\n",
    "\n",
    "1. **Very simple NN (fully manual forward calculations)**\n",
    "2. **Class-based model using `nn.Linear` layers**\n",
    "3. **Class-based model using `nn.Sequential`**\n",
    "\n",
    "All examples include:\n",
    "\n",
    "* A class defining the model\n",
    "* A forward function\n",
    "* Simple training loop template\n",
    "\n",
    "This will give you a **clear understanding of how PyTorch models are structured under the hood**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Class-Based Model: Very First Simple NN (Manual Weights)**\n",
    "\n",
    "Here we **manually create weights and do matrix multiplications ourselves**.\n",
    "\n",
    "##### üëâ Best for: understanding how PyTorch really works under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8892d614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3697],\n",
      "        [-1.2692],\n",
      "        [-1.1687],\n",
      "        [-1.0682]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Simple model with manually created parameters\n",
    "# ---------------------------------------------------\n",
    "class SimpleManualNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Manually define weights and biases\n",
    "        self.w1 = nn.Parameter(torch.randn(1, 4))\n",
    "        self.b1 = nn.Parameter(torch.randn(4))\n",
    "\n",
    "        self.w2 = nn.Parameter(torch.randn(4, 1))\n",
    "        self.b2 = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x @ self.w1 + self.b1    # Linear layer 1\n",
    "        h = F.relu(h)                # Activation\n",
    "        out = h @ self.w2 + self.b2  # Linear layer 2\n",
    "        return out\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = SimpleManualNN()\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_pred = model(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897b33d",
   "metadata": {},
   "source": [
    "‚úî Manually implemented linear layers\n",
    "‚úî Useful for learning core mechanics\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Class-Based Model Using `nn.Linear` (Proper PyTorch Way)**\n",
    "\n",
    "The cleanest and most common architecture structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73cc3092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4084],\n",
      "        [-0.4179],\n",
      "        [-0.4242],\n",
      "        [-0.4304]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Class model using nn.Linear layers\n",
    "# ---------------------------------------------------\n",
    "class LinearNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, 4)   # input ‚Üí hidden\n",
    "        self.fc2 = nn.Linear(4, 1)   # hidden ‚Üí output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = LinearNN()\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_pred = model(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eb0eb7",
   "metadata": {},
   "source": [
    "‚úî Clean and flexible\n",
    "‚úî Easy to expand with more layers\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Class-Based Model Using `nn.Sequential`**\n",
    "\n",
    "Here, the entire network is wrapped inside one Sequential block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7911f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0651],\n",
      "        [ 0.0861],\n",
      "        [ 0.1873],\n",
      "        [ 0.2884]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Class model using nn.Sequential\n",
    "# ---------------------------------------------------\n",
    "class SequentialNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "model = SequentialNN()\n",
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_pred = model(X)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b23d6c",
   "metadata": {},
   "source": [
    "‚úî Very compact\n",
    "‚úî Perfect for simple feedforward architectures\n",
    "\n",
    "---\n",
    "\n",
    "### **Training Loop Template (Works for All 3 Models)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cc2b929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 29.9272\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y = torch.tensor([[2.0], [4.0], [6.0], [8.0]])\n",
    "\n",
    "model = LinearNN()        # or SimpleManualNN(), SequentialNN()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "num_epochs = 5\n",
    "# ---------------------------\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = model(X)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch} Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd35729",
   "metadata": {},
   "source": [
    "### Summary Table (Easy Understanding)\n",
    "\n",
    "| Model Type         | How It's Built                                  | Best For                          |\n",
    "| ------------------ | ----------------------------------------------- | --------------------------------- |\n",
    "| **SimpleManualNN** | Manual weights (`nn.Parameter`) + manual matmul | Understanding internals           |\n",
    "| **LinearNN**       | Uses `nn.Linear` layers                         | Standard choice for most networks |\n",
    "| **SequentialNN**   | Uses `nn.Sequential` container                  | Very compact CNN/MLP blocks       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e9c29a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
