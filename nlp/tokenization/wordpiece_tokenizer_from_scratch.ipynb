{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86f3ab8",
   "metadata": {},
   "source": [
    "# WordPiece Tokenizer\n",
    "\n",
    "## üîπ What is WordPiece Tokenizer?\n",
    "\n",
    "**WordPiece** is a **subword-based tokenizer**, originally developed for **Google's BERT**.\n",
    "It aims to:\n",
    "\n",
    "* Handle unknown words (OOV = out-of-vocabulary),\n",
    "* Reduce vocabulary size,\n",
    "* Preserve semantic meaning as much as possible.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why WordPiece?\n",
    "\n",
    "Traditional tokenizers (like whitespace or word-based) fail in:\n",
    "\n",
    "* Handling rare/unknown words ‚Üí `[UNK]`\n",
    "* Needing a huge vocabulary to cover all possible words.\n",
    "\n",
    "‚úÖ **WordPiece** solves this by:\n",
    "\n",
    "* Breaking words into **known subword units**.\n",
    "* Leveraging a **data-driven** method to learn which subwords to merge.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ How it works ‚Äì Step by Step\n",
    "\n",
    "### 1. **Initial Vocabulary**\n",
    "\n",
    "* Start with a character-level vocabulary (`a` to `z`, digits, symbols, and special tokens like `[CLS]`, `[SEP]`, `[UNK]`, etc.)\n",
    "* Each word in the training corpus is represented as characters:\n",
    "\n",
    "  ```\n",
    "  \"hello\" ‚Üí ['h', 'e', 'l', 'l', 'o']\n",
    "         ‚Üí ['h', '##e', '##l', '##l', '##o']   # WordPiece adds '##' to mark continuation\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Build Vocabulary using Merge Rules**\n",
    "\n",
    "WordPiece is similar to Byte-Pair Encoding (BPE), but uses a **scoring function**:\n",
    "\n",
    "#### üîπ Merge Rule Scoring Formula:\n",
    "\n",
    "$$\n",
    "\\text{Score}(A, B) = \\frac{\\text{freq}(AB) \\times |Vocab|}{\\text{freq}(A) \\times \\text{freq}(B)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* `freq(AB)` = frequency of merged pair,\n",
    "* `|Vocab|` = current vocab size,\n",
    "* `freq(A)` and `freq(B)` = individual frequencies of A and B.\n",
    "\n",
    "Merge the best scoring pair iteratively until the vocabulary reaches the desired size.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Tokenization of New Sentences**\n",
    "\n",
    "To tokenize any input string:\n",
    "\n",
    "* Start from left to right.\n",
    "* Use **longest match first** (greedy).\n",
    "* If no match is found, use `[UNK]`.\n",
    "\n",
    "Example:\n",
    "Vocabulary: `[\"[CLS]\", \"[SEP]\", \"my\", \"name\", \"is\", \"saga\", \"##r\"]`\n",
    "\n",
    "Input: `\"my name is sagar\"`\n",
    "\n",
    "‚Üí Tokenized:\n",
    "\n",
    "```python\n",
    "[\"my\", \"name\", \"is\", \"saga\", \"##r\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Special Tokens**\n",
    "\n",
    "* `[CLS]` ‚Üí added at start\n",
    "* `[SEP]` ‚Üí added at end or between sentence pairs\n",
    "* `[PAD]` ‚Üí for padding\n",
    "* `[UNK]` ‚Üí unknown tokens\n",
    "\n",
    "So final tokenized form becomes:\n",
    "\n",
    "```\n",
    "[\"[CLS]\", \"my\", \"name\", \"is\", \"saga\", \"##r\", \"[SEP]\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example from BERT Tokenizer\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenizer.tokenize(\"unhappiness\"))\n",
    "\n",
    "# Output:\n",
    "# ['un', '##happiness']\n",
    "```\n",
    "\n",
    "BERT‚Äôs tokenizer knew that:\n",
    "\n",
    "* `un` is a common prefix,\n",
    "* `##happiness` is a known subword.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Pros and Cons\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "\n",
    "* Handles unknown words gracefully.\n",
    "* Compact vocab (e.g., BERT base uses \\~30,000 tokens).\n",
    "* Learns meaningful subwords (e.g., \"play\", \"##ing\", \"##er\").\n",
    "\n",
    "### ‚ùå Cons:\n",
    "\n",
    "* Requires pretraining on huge data.\n",
    "* Can break up rare or morphologically rich words awkwardly.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Summary Table\n",
    "\n",
    "| Step                   | Action                             |\n",
    "| ---------------------- | ---------------------------------- |\n",
    "| Init vocab             | Character-level                    |\n",
    "| Build word frequencies | From corpus                        |\n",
    "| Score token pairs      | Using scoring function             |\n",
    "| Merge best pairs       | Until vocab size is reached        |\n",
    "| Tokenize new sentence  | Using longest-match subword lookup |\n",
    "| Add special tokens     | \\[CLS], \\[SEP], etc.               |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e7a80",
   "metadata": {},
   "source": [
    "## Custom Implementation of Wordpiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e818be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "class CustomWordPieceTokenizer:\n",
    "    def __init__(self, vocab_size=100, special_tokens=None, max_len=16):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.vocab_inv = {}\n",
    "        self.word_freq = Counter()\n",
    "        self.word_tokens = {}\n",
    "        self.merge_rules = []\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.special_tokens = special_tokens or ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "        for i, token in enumerate(self.special_tokens):\n",
    "            self.vocab[token] = i\n",
    "        self.offset = len(self.special_tokens)\n",
    "\n",
    "    def train(self, corpus):\n",
    "        for sentence in corpus:\n",
    "            for word in sentence.strip().split():\n",
    "                self.word_freq[word] += 1\n",
    "\n",
    "        vocab_set = set()\n",
    "        for word in self.word_freq:\n",
    "            chars = list(word)\n",
    "            tokens = [chars[0]] + ['##' + c for c in chars[1:]]\n",
    "            self.word_tokens[word] = tokens\n",
    "            vocab_set.update(tokens)\n",
    "\n",
    "        self.vocab.update({tok: i + self.offset for i, tok in enumerate(sorted(vocab_set))})\n",
    "\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pair_freq = defaultdict(int)\n",
    "            token_freq = defaultdict(int)\n",
    "\n",
    "            for word, tokens in self.word_tokens.items():\n",
    "                freq = self.word_freq[word]\n",
    "                for token in tokens:\n",
    "                    token_freq[token] += freq\n",
    "                for i in range(len(tokens) - 1):\n",
    "                    pair = (tokens[i], tokens[i + 1])\n",
    "                    pair_freq[pair] += freq\n",
    "\n",
    "            if not pair_freq:\n",
    "                break\n",
    "\n",
    "            def score(pair):\n",
    "                ab = pair_freq[pair]\n",
    "                a = token_freq[pair[0]]\n",
    "                b = token_freq[pair[1]]\n",
    "                return (ab * len(self.vocab)) / (a * b + 1e-9)\n",
    "\n",
    "            best_pair = max(pair_freq, key=score)\n",
    "            new_token = best_pair[0] + best_pair[1].lstrip(\"##\")\n",
    "\n",
    "            self.vocab[new_token] = len(self.vocab)\n",
    "            self.merge_rules.append(best_pair)\n",
    "\n",
    "            for word in list(self.word_tokens):\n",
    "                tokens = self.word_tokens[word]\n",
    "                i = 0\n",
    "                while i < len(tokens) - 1:\n",
    "                    if (tokens[i], tokens[i + 1]) == best_pair:\n",
    "                        tokens[i:i + 2] = [new_token]\n",
    "                    else:\n",
    "                        i += 1\n",
    "\n",
    "    def save_vocab(self, path='vocab.json'):\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.vocab, f, indent=2)\n",
    "\n",
    "    def load_vocab(self, path='vocab.json'):\n",
    "        with open(path, 'r') as f:\n",
    "            self.vocab = json.load(f)\n",
    "        self.vocab_inv = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        tokens = []\n",
    "        for word in sentence.strip().split():\n",
    "            tokens.extend(self._wordpiece_tokenize(word))\n",
    "        return tokens\n",
    "\n",
    "    def _wordpiece_tokenize(self, word):\n",
    "        chars = list(word)\n",
    "        if len(chars) == 1:\n",
    "            tokens = chars\n",
    "        else:\n",
    "            tokens = [chars[0]] + ['##' + c for c in chars[1:]]\n",
    "\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            new_token = tokens[i] + tokens[i + 1].lstrip(\"##\")\n",
    "            if new_token in self.vocab:\n",
    "                tokens[i:i + 2] = [new_token]\n",
    "            else:\n",
    "                i += 1\n",
    "        return [tok if tok in self.vocab else '[UNK]' for tok in tokens]\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.vocab.get(tok, self.vocab['[UNK]']) for tok in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        if not self.vocab_inv:\n",
    "            self.vocab_inv = {v: k for k, v in self.vocab.items()}\n",
    "        return [self.vocab_inv.get(i, '[UNK]') for i in ids]\n",
    "\n",
    "    def __call__(self, sentence, padding=False, truncation=False, return_tensors=None):\n",
    "        tokens = ['[CLS]'] + self.tokenize(sentence) + ['[SEP]']\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        if truncation and len(input_ids) > self.max_len:\n",
    "            input_ids = input_ids[:self.max_len]\n",
    "\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "\n",
    "        if padding and len(input_ids) < self.max_len:\n",
    "            pad_len = self.max_len - len(input_ids)\n",
    "            input_ids += [self.vocab['[PAD]']] * pad_len\n",
    "            attention_mask += [0] * pad_len\n",
    "            token_type_ids += [0] * pad_len\n",
    "\n",
    "        output = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "        if return_tensors == \"pt\":\n",
    "            import torch\n",
    "            for k in output:\n",
    "                output[k] = torch.tensor(output[k]).unsqueeze(0)  # Add batch dim\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fe7dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['h', '##i', 'm', '##y', 'n', '##a', '##m', '##e', 'i', '##s', 's', '##a', '##g', '##ar']\n",
      "Token IDs: [53, 20, 57, 35, 58, 12, 23, 16, 54, 29, 62, 12, 18, 451]\n",
      "Back to Tokens: ['h', '##i', 'm', '##y', 'n', '##a', '##m', '##e', 'i', '##s', 's', '##a', '##g', '##ar']\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "corpus = [\n",
    "    \"Artificial Intelligence (AI) refers to the technology that allows machines and computers to replicate human intelligence. Enables systems to perform tasks that require human-like decision-making, such as learning from data, identifying patterns, making informed choices and solving complex problems. Improves continuously by utilizing methods like machine learning and deep learning. Used in healthcare for diagnosing diseases, finance for fraud detection, e-commerce for personalized recommendations and transportation for self-driving cars. It also powers virtual assistants like Siri and Alexa, chatbots for customer support and manufacturing robots that automate production processes.\",\n",
    "    \"Machine Learning is a subset of artificial intelligence (AI) that focuses on building systems that can learn from and make decisions based on data. Instead of being explicitly programmed to perform a task, a machine learning model uses algorithms to identify patterns within data and improve its performance over time without human intervention.\",\n",
    "    \"Generative AI refers to a type of artificial intelligence designed to create new content, whether it's text, images, music, or even video. Unlike traditional AI, which typically focuses on analyzing and classifying data, generative AI goes a step further by using patterns it has learned from large datasets to generate new, original outputs. Essentially, it creates rather than just recognizes.\"\n",
    "]\n",
    "\n",
    "tokenizer = CustomWordPieceTokenizer(vocab_size=512)\n",
    "tokenizer.train(corpus)\n",
    "tokenizer.save_vocab('vocab.json')  # Save vocab\n",
    "\n",
    "# Load and test\n",
    "tokenizer.load_vocab('vocab.json')\n",
    "x = tokenizer.tokenize(\"hi my name is sagar\")\n",
    "print(\"Tokens:\", x)\n",
    "\n",
    "y = tokenizer.convert_tokens_to_ids(x)\n",
    "print(\"Token IDs:\", y)\n",
    "\n",
    "z = tokenizer.convert_ids_to_tokens(y)\n",
    "print(\"Back to Tokens:\", z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1be319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: {'input_ids': tensor([[  2,  53,  20,  57,  35,  58,  12,  23,  16,  54,  29,  62,  12,  18,\n",
      "         451,   3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "z1: ['[CLS]', 'h', '##i', 'm', '##y', 'n', '##a', '##m', '##e', 'i', '##s', 's', '##a', '##g', '##ar', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = CustomWordPieceTokenizer(vocab_size=50)\n",
    "tokenizer.load_vocab('vocab.json')\n",
    "\n",
    "x1 = tokenizer(\"hi my name is sagar\", padding=True, truncation=True, return_tensors='pt')\n",
    "print(\"x1:\", x1)\n",
    "\n",
    "z1 = tokenizer.convert_ids_to_tokens(x1[\"input_ids\"][0].tolist())\n",
    "print(\"z1:\", z1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2523299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ab434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "\n",
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab_size=100, special_tokens=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.ids_to_tokens = {}\n",
    "        self.token_freq = Counter()\n",
    "        self.word_tokens = {}\n",
    "        self.merge_rules = []\n",
    "        self.special_tokens = special_tokens or {\n",
    "            \"[PAD]\": 0,\n",
    "            \"[CLS]\": 101,\n",
    "            \"[SEP]\": 102,\n",
    "            \"[UNK]\": 100\n",
    "        }\n",
    "        self.vocab.update(self.special_tokens)\n",
    "        self.ids_to_tokens = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def train(self, corpus):\n",
    "        word_freq = Counter()\n",
    "        for sentence in corpus:\n",
    "            for word in sentence.strip().split():\n",
    "                word_freq[word] += 1\n",
    "\n",
    "        # Step 1: Initialize characters as tokens\n",
    "        for word in word_freq:\n",
    "            chars = list(word)\n",
    "            tokens = [chars[0]] + ['##' + c for c in chars[1:]]\n",
    "            self.word_tokens[word] = tokens\n",
    "            for token in tokens:\n",
    "                self.token_freq[token] += word_freq[word]\n",
    "\n",
    "        # Step 2: Merge most frequent pairs\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pair_freq = defaultdict(int)\n",
    "            for word, tokens in self.word_tokens.items():\n",
    "                freq = word_freq[word]\n",
    "                for i in range(len(tokens) - 1):\n",
    "                    pair = (tokens[i], tokens[i + 1])\n",
    "                    pair_freq[pair] += freq\n",
    "\n",
    "            if not pair_freq:\n",
    "                break\n",
    "\n",
    "            best_pair = self._score_best_pair(pair_freq)\n",
    "            if not best_pair:\n",
    "                break\n",
    "\n",
    "            new_token = best_pair[0] + best_pair[1].lstrip(\"##\")\n",
    "            self.merge_rules.append(best_pair)\n",
    "\n",
    "            for word in self.word_tokens:\n",
    "                tokens = self.word_tokens[word]\n",
    "                i = 0\n",
    "                while i < len(tokens) - 1:\n",
    "                    if (tokens[i], tokens[i + 1]) == best_pair:\n",
    "                        tokens[i:i + 2] = [new_token]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                self.word_tokens[word] = tokens\n",
    "\n",
    "            self.token_freq[new_token] = sum(\n",
    "                word_freq[word] for word, tokens in self.word_tokens.items() if new_token in tokens\n",
    "            )\n",
    "\n",
    "            if new_token not in self.vocab:\n",
    "                idx = len(self.vocab)\n",
    "                self.vocab[new_token] = idx\n",
    "                self.ids_to_tokens[idx] = new_token\n",
    "\n",
    "    def _score_best_pair(self, pair_freq):\n",
    "        best_score = -1\n",
    "        best_pair = None\n",
    "        for (a, b), ab_freq in pair_freq.items():\n",
    "            score = ab_freq / (self.token_freq[a] * self.token_freq[b] + 1e-9)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pair = (a, b)\n",
    "        return best_pair\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "        for word in text.strip().split():\n",
    "            tokens.extend(self._word_to_subtokens(word))\n",
    "        return tokens\n",
    "\n",
    "    def _word_to_subtokens(self, word):\n",
    "        chars = list(word)\n",
    "        tokens = [chars[0]] + ['##' + c for c in chars[1:]]\n",
    "        i = 0\n",
    "        result = []\n",
    "        while i < len(tokens):\n",
    "            for j in range(len(tokens), i, -1):\n",
    "                subword = ''.join(tokens[i:j])\n",
    "                if subword in self.vocab:\n",
    "                    result.append(subword)\n",
    "                    i = j\n",
    "                    break\n",
    "            else:\n",
    "                result.append(\"[UNK]\")\n",
    "                break\n",
    "        return result\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.vocab.get(t, self.vocab[\"[UNK]\"]) for t in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.ids_to_tokens.get(i, \"[UNK]\") for i in ids]\n",
    "\n",
    "    def __call__(self, text, padding=False, truncation=False, return_tensors=None, max_length=20):\n",
    "        tokens = [\"[CLS]\"] + self.tokenize(text) + [\"[SEP]\"]\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        if padding and len(input_ids) < max_length:\n",
    "            pad_len = max_length - len(input_ids)\n",
    "            input_ids += [self.vocab[\"[PAD]\"]] * pad_len\n",
    "            attention_mask += [0] * pad_len\n",
    "            token_type_ids += [0] * pad_len\n",
    "\n",
    "        if truncation and len(input_ids) > max_length:\n",
    "            input_ids = input_ids[:max_length]\n",
    "            attention_mask = attention_mask[:max_length]\n",
    "            token_type_ids = token_type_ids[:max_length]\n",
    "\n",
    "        encoded = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "        if return_tensors == \"pt\":\n",
    "            encoded = {k: torch.tensor(v).unsqueeze(0) for k, v in encoded.items()}\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def save(self, path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        with open(os.path.join(path, \"tokenizer.json\"), \"w\") as f:\n",
    "            json.dump({\n",
    "                \"vocab\": self.vocab,\n",
    "                \"merge_rules\": self.merge_rules\n",
    "            }, f, indent=2)\n",
    "\n",
    "        with open(os.path.join(path, \"tokenizer_config.json\"), \"w\") as f:\n",
    "            json.dump({\n",
    "                \"tokenizer_class\": \"WordPieceTokenizer\",\n",
    "                \"do_lower_case\": True,\n",
    "                \"model_max_length\": 512\n",
    "            }, f, indent=2)\n",
    "\n",
    "        with open(os.path.join(path, \"special_tokens_map.json\"), \"w\") as f:\n",
    "            json.dump(self.special_tokens, f, indent=2)\n",
    "\n",
    "    def load(self, path):\n",
    "        with open(os.path.join(path, \"tokenizer.json\")) as f:\n",
    "            data = json.load(f)\n",
    "            self.vocab = data[\"vocab\"]\n",
    "            self.merge_rules = data[\"merge_rules\"]\n",
    "            self.ids_to_tokens = {int(v): k for k, v in self.vocab.items()}\n",
    "\n",
    "        with open(os.path.join(path, \"special_tokens_map.json\")) as f:\n",
    "            self.special_tokens = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3755156a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "corpus = [\n",
    "    \"Artificial Intelligence (AI) refers to the technology that allows machines and computers to replicate human intelligence. Enables systems to perform tasks that require human-like decision-making, such as learning from data, identifying patterns, making informed choices and solving complex problems. Improves continuously by utilizing methods like machine learning and deep learning. Used in healthcare for diagnosing diseases, finance for fraud detection, e-commerce for personalized recommendations and transportation for self-driving cars. It also powers virtual assistants like Siri and Alexa, chatbots for customer support and manufacturing robots that automate production processes.\",\n",
    "    \"Machine Learning is a subset of artificial intelligence (AI) that focuses on building systems that can learn from and make decisions based on data. Instead of being explicitly programmed to perform a task, a machine learning model uses algorithms to identify patterns within data and improve its performance over time without human intervention.\",\n",
    "    \"Generative AI refers to a type of artificial intelligence designed to create new content, whether it's text, images, music, or even video. Unlike traditional AI, which typically focuses on analyzing and classifying data, generative AI goes a step further by using patterns it has learned from large datasets to generate new, original outputs. Essentially, it creates rather than just recognizes.\"\n",
    "]\n",
    "\n",
    "tokenizer = WordPieceTokenizer(vocab_size=50)\n",
    "tokenizer.train(corpus)\n",
    "print(tokenizer(\"hello general\", return_tensors=\"pt\"))\n",
    "\n",
    "# Save tokenizer files\n",
    "tokenizer.save(\"my_tokenizer\")\n",
    "\n",
    "# Reload\n",
    "tokenizer2 = WordPieceTokenizer()\n",
    "tokenizer2.load(\"my_tokenizer\")\n",
    "print(tokenizer2(\"hello general\", return_tensors=\"pt\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
