{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580f3480",
   "metadata": {},
   "source": [
    "# ‚úÖ Byte Pair Encoding (BPE) Tokenizer \n",
    "\n",
    "The **BPE tokenizer** (Byte Pair Encoding) is a subword tokenization algorithm that merges frequent character pairs to form subword units. It sits between word-level and character-level tokenization, enabling better generalization and efficient vocabulary size control ‚Äî especially useful in **language models** like GPT, RoBERTa, and CLIP.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Why Use BPE?\n",
    "\n",
    "* **Solves out-of-vocabulary (OOV)** problem by breaking rare words into known subword units.\n",
    "* **Reduces vocabulary size** compared to word-level tokenizers.\n",
    "* **Captures frequent subwords**, helpful for morphologically rich languages.\n",
    "* **Balances between character and word-level modeling**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Intuition Behind BPE\n",
    "\n",
    "Imagine your corpus contains the following text:\n",
    "\n",
    "```\n",
    "\"low lower lowest\"\n",
    "```\n",
    "\n",
    "1. Start with a vocabulary of characters: `l`, `o`, `w`, `e`, `r`, `s`, `t`.\n",
    "2. Represent each word as a sequence of characters ending with a special end-of-word symbol `</w>`:\n",
    "\n",
    "   ```\n",
    "   l o w </w>\n",
    "   l o w e r </w>\n",
    "   l o w e s t </w>\n",
    "   ```\n",
    "3. Count frequency of adjacent symbol pairs.\n",
    "4. Merge the **most frequent pair** into a new symbol (e.g., `l o` ‚Üí `lo`).\n",
    "5. Repeat the merge process a fixed number of times (e.g., 10000 steps).\n",
    "6. The final vocabulary contains original characters + frequent subword units like `low`, `er`, `est`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Step-by-Step Algorithm\n",
    "\n",
    "### üõ†Ô∏è 1. Prepare Initial Vocabulary\n",
    "\n",
    "* Split all words in your corpus into individual characters.\n",
    "* Add a special end-of-word marker like `</w>`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "\"lower\" ‚Üí [\"l\", \"o\", \"w\", \"e\", \"r\", \"</w>\"]\n",
    "```\n",
    "\n",
    "### üîÅ 2. Iteratively Merge Frequent Pairs\n",
    "\n",
    "* Count frequencies of adjacent pairs (e.g., `('l','o')`, `('o','w')`).\n",
    "* Merge the most frequent pair into a new token.\n",
    "* Update all words accordingly.\n",
    "* Repeat for `N` merge operations.\n",
    "\n",
    "### üßæ 3. Final Vocabulary\n",
    "\n",
    "* Includes all characters + frequent merged tokens.\n",
    "* Vocabulary is compact but expressive.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Tokenization Process (After Training)\n",
    "\n",
    "Given a new word like `\"lowered\"`:\n",
    "\n",
    "1. Try to match the longest possible subwords from your learned vocabulary.\n",
    "2. Greedily tokenize:\n",
    "\n",
    "   ```\n",
    "   \"lowered\" ‚Üí [\"low\", \"er\", \"ed\"]\n",
    "   ```\n",
    "\n",
    "If `\"lowered\"` is not in vocab:\n",
    "\n",
    "```\n",
    "‚Üí \"low\", \"er\", \"##e\", \"##d\"  (greedy fallback to smaller subunits)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìÑ Output Files (Used in Transformers and HuggingFace)\n",
    "\n",
    "When exporting a BPE tokenizer for reuse, we usually generate:\n",
    "\n",
    "| File                      | Description                                        |\n",
    "| ------------------------- | -------------------------------------------------- |\n",
    "| `tokenizer.json`          | Contains vocab, merges, and rules                  |\n",
    "| `tokenizer_config.json`   | Contains casing, tokenizer type, settings          |\n",
    "| `special_tokens_map.json` | Maps special tokens like `[PAD]`, `[CLS]`, `[SEP]` |\n",
    "\n",
    "---\n",
    "\n",
    "## üìò Pros & Cons of BPE\n",
    "\n",
    "### ‚úÖ Pros:\n",
    "\n",
    "* Handles unknown words gracefully (subword fallback).\n",
    "* Efficient vocab usage.\n",
    "* Simple to implement and fast.\n",
    "\n",
    "### ‚ùå Cons:\n",
    "\n",
    "* Greedy merging may miss semantically meaningful splits.\n",
    "* Merge decisions are purely frequency-based.\n",
    "* Doesn't handle multilingual edge cases as well as SentencePiece/Unigram.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† BPE vs WordPiece vs Unigram\n",
    "\n",
    "| Feature        | BPE                | WordPiece                      | Unigram                     |\n",
    "| -------------- | ------------------ | ------------------------------ | --------------------------- |\n",
    "| Merge Strategy | Greedy pair merges | Greedy + frequency-based score | Probabilistic over subwords |\n",
    "| Token Growth   | Linear             | Controlled                     | Probabilistic               |\n",
    "| Model Examples | GPT-2, RoBERTa     | BERT, ALBERT                   | XLNet, T5, MT5              |\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ In Practice\n",
    "\n",
    "Popular models using BPE:\n",
    "\n",
    "* **GPT-2/GPT-3/GPT-4** (OpenAI): Uses BPE via byte encoding.\n",
    "* **RoBERTa**: Uses byte-level BPE tokenizer.\n",
    "* **CLIP** (OpenAI): Also uses byte-level BPE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ccf388",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929f71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict, Counter\n",
    "import torch\n",
    "import regex as re\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size=100, special_tokens=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = {}\n",
    "        self.ids_to_tokens = {}\n",
    "        self.merge_rules = {}  # Maps pairs to their merged token\n",
    "        self.word_frequencies = Counter()\n",
    "        self.special_tokens = special_tokens or {\n",
    "            \"[PAD]\": 0,\n",
    "            \"[CLS]\": 1,\n",
    "            \"[SEP]\": 2,\n",
    "            \"[UNK]\": 3,\n",
    "            \"[MASK]\": 4\n",
    "        }\n",
    "        self.vocab.update(self.special_tokens)\n",
    "        self.ids_to_tokens = {v: k for k, v in self.vocab.items()}\n",
    "        self.pattern = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "        \n",
    "    def train(self, corpus, min_frequency=2):\n",
    "        # Pre-tokenize the corpus into words\n",
    "        words = []\n",
    "        for sentence in corpus:\n",
    "            words.extend(self._pre_tokenize(sentence))\n",
    "        \n",
    "        # Count word frequencies\n",
    "        self.word_frequencies = Counter(words)\n",
    "        \n",
    "        # Initialize vocabulary with characters\n",
    "        vocab = set()\n",
    "        for word in self.word_frequencies:\n",
    "            for char in word:\n",
    "                vocab.add(char)\n",
    "        \n",
    "        # Convert to list and sort\n",
    "        vocab = sorted(vocab)\n",
    "        \n",
    "        # Initialize vocabulary with bytes\n",
    "        self.vocab = {token: i for i, token in enumerate(vocab)}\n",
    "        self.vocab.update(self.special_tokens)\n",
    "        self.ids_to_tokens = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "        # Get initial pairs\n",
    "        pairs = defaultdict(int)\n",
    "        for word, freq in self.word_frequencies.items():\n",
    "            symbols = list(word)\n",
    "            for i in range(len(symbols)-1):\n",
    "                pairs[(symbols[i], symbols[i+1])] += freq\n",
    "        \n",
    "        # BPE algorithm\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            if not pairs:\n",
    "                break\n",
    "            \n",
    "            # Get most frequent pair\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            best_freq = pairs[best_pair]\n",
    "            \n",
    "            if best_freq < min_frequency:\n",
    "                break\n",
    "                \n",
    "            # Merge the pair\n",
    "            new_token = \"\".join(best_pair)\n",
    "            self.vocab[new_token] = len(self.vocab)\n",
    "            self.ids_to_tokens[len(self.vocab)-1] = new_token\n",
    "            self.merge_rules[best_pair] = new_token\n",
    "            \n",
    "            # Update pairs\n",
    "            new_pairs = defaultdict(int)\n",
    "            for word, freq in self.word_frequencies.items():\n",
    "                new_word = self._merge_pair_in_word(word, best_pair, new_token)\n",
    "                if new_word != word:\n",
    "                    symbols = new_word.split()\n",
    "                    for i in range(len(symbols)-1):\n",
    "                        new_pairs[(symbols[i], symbols[i+1])] += freq\n",
    "            \n",
    "            # Remove the merged pair and add new pairs\n",
    "            del pairs[best_pair]\n",
    "            for pair in new_pairs:\n",
    "                pairs[pair] += new_pairs[pair]\n",
    "    \n",
    "    def _pre_tokenize(self, text):\n",
    "        # Simple whitespace tokenizer with regex pattern\n",
    "        tokens = []\n",
    "        for token in re.findall(self.pattern, text):\n",
    "            tokens.append(token)\n",
    "        return tokens\n",
    "    \n",
    "    def _merge_pair_in_word(self, word, pair, new_token):\n",
    "        symbols = word.split()\n",
    "        i = 0\n",
    "        while i < len(symbols)-1:\n",
    "            if symbols[i] == pair[0] and symbols[i+1] == pair[1]:\n",
    "                symbols[i:i+2] = [new_token]\n",
    "            else:\n",
    "                i += 1\n",
    "        return \" \".join(symbols)\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        tokens = self._pre_tokenize(text)\n",
    "        bpe_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            # Convert token to string of characters separated by spaces\n",
    "            chars = list(token)\n",
    "            current_word = \" \".join(chars)\n",
    "            \n",
    "            # Apply all merge rules\n",
    "            for pair, merged in self.merge_rules.items():\n",
    "                while True:\n",
    "                    new_word = self._merge_pair_in_word(current_word, pair, merged)\n",
    "                    if new_word == current_word:\n",
    "                        break\n",
    "                    current_word = new_word\n",
    "            \n",
    "            # Split the final merged word\n",
    "            bpe_tokens.extend(current_word.split())\n",
    "        \n",
    "        return bpe_tokens\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.vocab.get(token, self.vocab[\"[UNK]\"]) for token in tokens]\n",
    "    \n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.ids_to_tokens.get(i, \"[UNK]\") for i in ids]\n",
    "    \n",
    "    def __call__(self, text, padding=False, truncation=False, return_tensors=None, max_length=20):\n",
    "        tokens = [\"[CLS]\"] + self.tokenize(text) + [\"[SEP]\"]\n",
    "        input_ids = self.convert_tokens_to_ids(tokens)\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        if padding and len(input_ids) < max_length:\n",
    "            pad_len = max_length - len(input_ids)\n",
    "            input_ids += [self.vocab[\"[PAD]\"]] * pad_len\n",
    "            attention_mask += [0] * pad_len\n",
    "            token_type_ids += [0] * pad_len\n",
    "\n",
    "        if truncation and len(input_ids) > max_length:\n",
    "            input_ids = input_ids[:max_length]\n",
    "            attention_mask = attention_mask[:max_length]\n",
    "            token_type_ids = token_type_ids[:max_length]\n",
    "\n",
    "        encoded = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"attention_mask\": attention_mask\n",
    "        }\n",
    "\n",
    "        if return_tensors == \"pt\":\n",
    "            encoded = {k: torch.tensor(v).unsqueeze(0) for k, v in encoded.items()}\n",
    "\n",
    "        return encoded\n",
    "    \n",
    "    def save(self, path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        with open(os.path.join(path, \"tokenizer.json\"), \"w\") as f:\n",
    "            json.dump({\n",
    "                \"vocab\": self.vocab,\n",
    "                \"merge_rules\": self.merge_rules,\n",
    "                \"word_frequencies\": self.word_frequencies\n",
    "            }, f, indent=2)\n",
    "\n",
    "        with open(os.path.join(path, \"tokenizer_config.json\"), \"w\") as f:\n",
    "            json.dump({\n",
    "                \"tokenizer_class\": \"BPETokenizer\",\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"model_max_length\": 512\n",
    "            }, f, indent=2)\n",
    "\n",
    "        with open(os.path.join(path, \"special_tokens_map.json\"), \"w\") as f:\n",
    "            json.dump(self.special_tokens, f, indent=2)\n",
    "    \n",
    "    def load(self, path):\n",
    "        with open(os.path.join(path, \"tokenizer.json\")) as f:\n",
    "            data = json.load(f)\n",
    "            self.vocab = {k: int(v) for k, v in data[\"vocab\"].items()}\n",
    "            self.merge_rules = {tuple(k): v for k, v in data[\"merge_rules\"].items()}\n",
    "            self.word_frequencies = Counter(data[\"word_frequencies\"])\n",
    "            self.ids_to_tokens = {int(v): k for k, v in self.vocab.items()}\n",
    "\n",
    "        with open(os.path.join(path, \"tokenizer_config.json\")) as f:\n",
    "            config = json.load(f)\n",
    "            self.vocab_size = config.get(\"vocab_size\", len(self.vocab))\n",
    "\n",
    "        with open(os.path.join(path, \"special_tokens_map.json\")) as f:\n",
    "            self.special_tokens = json.load(f)\n",
    "            # Update vocab with any new special tokens\n",
    "            for token, idx in self.special_tokens.items():\n",
    "                if token not in self.vocab:\n",
    "                    self.vocab[token] = idx\n",
    "                    self.ids_to_tokens[idx] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the tokenizer\n",
    "corpus = [\n",
    "    \"Artificial Intelligence (AI) refers to the technology that allows machines and computers to replicate human intelligence. Enables systems to perform tasks that require human-like decision-making, such as learning from data, identifying patterns, making informed choices and solving complex problems. Improves continuously by utilizing methods like machine learning and deep learning. Used in healthcare for diagnosing diseases, finance for fraud detection, e-commerce for personalized recommendations and transportation for self-driving cars. It also powers virtual assistants like Siri and Alexa, chatbots for customer support and manufacturing robots that automate production processes.\",\n",
    "    \"Machine Learning is a subset of artificial intelligence (AI) that focuses on building systems that can learn from and make decisions based on data. Instead of being explicitly programmed to perform a task, a machine learning model uses algorithms to identify patterns within data and improve its performance over time without human intervention.\",\n",
    "    \"Generative AI refers to a type of artificial intelligence designed to create new content, whether it's text, images, music, or even video. Unlike traditional AI, which typically focuses on analyzing and classifying data, generative AI goes a step further by using patterns it has learned from large datasets to generate new, original outputs. Essentially, it creates rather than just recognizes.\"\n",
    "]\n",
    "\n",
    "tokenizer = BPETokenizer(vocab_size=50)\n",
    "tokenizer.train(corpus)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(\"my_bpe_tokenizer\")\n",
    "\n",
    "# Load the tokenizer\n",
    "new_tokenizer = BPETokenizer()\n",
    "new_tokenizer.load(\"my_bpe_tokenizer\")\n",
    "\n",
    "# Tokenize a new sentence\n",
    "text = \"This is a test sentence.\"\n",
    "tokens = new_tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "ids = new_tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(\"Token IDs:\", ids)\n",
    "\n",
    "# Full encoding\n",
    "encoded = new_tokenizer(text, padding=True, max_length=10)\n",
    "print(\"Encoded:\", encoded)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
